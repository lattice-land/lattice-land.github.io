<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Lattice Land Book</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="CUDA-Battery.html"><strong aria-hidden="true">2.</strong> CUDA-Battery Library</a></li><li class="chapter-item expanded affix "><li class="part-title">Parallel Lattice Programming</li><li class="chapter-item expanded "><a href="minimum.html"><strong aria-hidden="true">3.</strong> Minimum Algorithm</a></li><li class="chapter-item expanded "><a href="floyd-warshall.html"><strong aria-hidden="true">4.</strong> Floyd-Warshall Algorithm</a></li><li class="chapter-item expanded affix "><li class="part-title">Abstract Constraint Reasoning</li><li class="chapter-item expanded affix "><li class="part-title">Parallel Abstract Constraint Reasoning</li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">Lattice Land Book</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Hello! You just arrived in lattice land!
Lattice land is a collection of C++ libraries compatible with NVIDIA CUDA framework.
Most of the libraries implement <em>abstract domains for constraint reasoning</em> a new kind of data structure based on <a href="https://en.wikipedia.org/wiki/Abstract_interpretation">abstract interpretation</a>, <a href="https://en.wikipedia.org/wiki/Lattice_(order)">lattice theory</a> and <a href="https://en.wikipedia.org/wiki/Constraint_satisfaction">constraint reasoning</a>.
This is a book presenting our research project, the involved research papers and the code documentation.</p>
<p>This project is available on <a href="https://github.com/lattice-land">github</a>.</p>
<h2 id="documentation-of-libraries"><a class="header" href="#documentation-of-libraries">Documentation of libraries</a></h2>
<ul>
<li><a href="https://lattice-land.github.io/cuda-battery">cuda-battery</a>: Memory allocators, vector, utilities and more which run on both CPU and GPU (CUDA).
See also the chapter <a href="CUDA-Battery.html">CUDA-Battery Library</a>.</li>
<li><a href="https://lattice-land.github.io/lala-core">lala-core</a>: Core definitions of the formula AST and abstract domain.</li>
<li><a href="https://github.com/lattice-land/lattice-land.github.io">lattice-land.github.io</a>: The repository hosting this book and the libraries documentation.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cuda-battery-library"><a class="header" href="#cuda-battery-library">CUDA-Battery Library</a></h1>
<h2 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h2>
<ul>
<li>Namespace: <code>battery::*</code>.</li>
<li>The documentation is not exhaustive (which is why we provide a link to the standard C++ STL documentation), but we document most of the main differences and the features without a standard counterpart.</li>
<li>The table below is a quick reference to the most useful features, but it is not exhaustive.</li>
<li><em>The structures provided here are not thread-safe, this responsibility is delegated to the user of this library.</em></li>
</ul>
<table><thead><tr><th>Category</th><th>Main features</th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><a href="cuda-battery/allocator_8hpp.html">Allocator</a></td><td><a href="cuda-battery/classbattery_1_1standard__allocator.html"><code>standard_allocator</code></a></td><td><a href="cuda-battery/classbattery_1_1global__allocator.html"><code>global_allocator</code></a></td><td><a href="cuda-battery/classbattery_1_1managed__allocator.html"><code>managed_allocator</code></a></td><td><a href="cuda-battery/classbattery_1_1pool__allocator.html"><code>pool_allocator</code></a></td></tr>
<tr><td>Pointers</td><td><a href="cuda-battery/classbattery_1_1shared__ptr.html"><code>shared_ptr</code></a> (<a href="https://en.cppreference.com/w/cpp/memory/shared_ptr"><code>std</code></a>)</td><td><a href="cuda-battery/namespacebattery.html#a51fbd102c2aef01adc744cee4bc35ea9"><code>make_shared</code></a> (<a href="https://en.cppreference.com/w/cpp/memory/shared_ptr/make_shared"><code>std</code></a>)</td><td><a href="cuda-battery/namespacebattery.html#aee9241d882f78f0130435b46389ff9ac"><code>allocate_shared</code></a> (<a href="https://en.cppreference.com/w/cpp/memory/shared_ptr/allocate_shared"><code>std</code></a>)</td><td></td></tr>
<tr><td></td><td><a href="cuda-battery/classbattery_1_1unique__ptr.html"><code>unique_ptr</code></a> (<a href="https://en.cppreference.com/w/cpp/memory/unique_ptr"><code>std</code></a>)</td><td><a href="cuda-battery/namespacebattery.html#aa354aeb0995d495b2b9858a6ea2fb568"><code>make_unique</code></a> (<a href="https://en.cppreference.com/w/cpp/memory/unique_ptr/make_unique"><code>std</code></a>)</td><td><a href="cuda-battery/namespacebattery.html#aa781d8577b63b6e7b789223e825f52c3"><code>make_unique_block</code></a></td><td><a href="cuda-battery/namespacebattery.html#a5ba4adb6d7953ca7e8c6602b7e455b14"><code>make_unique_grid</code></a></td></tr>
<tr><td>Containers</td><td><a href="cuda-battery/vector_8hpp.html"><code>vector</code></a> (<a href="https://en.cppreference.com/w/cpp/container/vector"><code>std</code></a>)</td><td><a href="cuda-battery/string_8hpp.html"><code>string</code></a> (<a href="https://en.cppreference.com/w/cpp/string/basic_string"><code>std</code></a>)</td><td></td><td></td></tr>
<tr><td></td><td><a href="https://en.cppreference.com/w/cpp/utility/tuple"><code>tuple</code></a></td><td><a href="cuda-battery/variant_8hpp.html"><code>variant</code></a> (<a href="https://en.cppreference.com/w/cpp/utility/variant"><code>std</code></a>)</td><td><a href="cuda-battery/bitset_8hpp.html"><code>bitset</code></a> (<a href="https://en.cppreference.com/w/cpp/utility/bitset"><code>std</code></a>)</td><td></td></tr>
<tr><td>Utility</td><td><a href="cuda-battery/utility_8hpp.html#a7b01e29f669d6beed251f1b2a547ca93"><code>CUDA</code></a></td><td><a href="cuda-battery/utility_8hpp.html#a2eb6f9e0395b47b8d5e3eeae4fe0c116"><code>INLINE</code></a></td><td><a href="cuda-battery/utility_8hpp.html#a289596c1db721f82251de6902f9699db"><code>CUDAE</code></a></td><td><a href="cuda-battery/utility_8hpp.html#af35c92d967acfadd086658422f631100"><code>CUDAEX</code></a></td></tr>
<tr><td></td><td><a href="cuda-battery/structbattery_1_1limits.html"><code>limits</code></a></td><td><a href="cuda-battery/namespacebattery.html#a7fdea425c76eab201a009ea09b8cbac0"><code>ru_cast</code></a></td><td><a href="cuda-battery/namespacebattery.html#aa2296c962277e71780bccf1ba9708f59"><code>rd_cast</code></a></td><td></td></tr>
<tr><td></td><td><a href="cuda-battery/namespacebattery.html#a2821ae67e8ea81b375f3fd6d70909fef"><code>popcount</code></a> (<a href="https://en.cppreference.com/w/cpp/numeric/popcount"><code>std</code></a>)</td><td><a href="cuda-battery/namespacebattery.html#aa18a34122dc3e8b7e96c4a54eeffa9aa"><code>countl_zero</code></a> (<a href="https://en.cppreference.com/w/cpp/numeric/countl_zero"><code>std</code></a>)</td><td><a href="cuda-battery/namespacebattery.html#ae252a50e577d7b092eb368b7e0289772"><code>countl_one</code></a> (<a href="https://en.cppreference.com/w/cpp/numeric/countl_one"><code>std</code></a>)</td><td><a href="cuda-battery/namespacebattery.html#a7338f90fab224e49c3716c5eace58bee"><code>countr_zero</code></a> (<a href="https://en.cppreference.com/w/cpp/numeric/countr_zero"><code>std</code></a>)</td></tr>
<tr><td></td><td><a href="cuda-battery/namespacebattery.html#a974d0a682d546e1185ae7dca29c272d6"><code>countr_one</code></a> (<a href="https://en.cppreference.com/w/cpp/numeric/countr_one"><code>std</code></a>)</td><td><a href="cuda-battery/namespacebattery.html#a31b3f5ee3799a73d29c153ebd222cdea"><code>signum</code></a></td><td><a href="cuda-battery/namespacebattery.html#a93472d80842253624e2436eef7b900b6"><code>ipow</code></a></td><td></td></tr>
<tr><td></td><td><a href="cuda-battery/namespacebattery.html#af3a4582a08267940dbdb5b39044aa4c6"><code>add_up</code></a></td><td><a href="cuda-battery/namespacebattery.html#a43d013f1db8f8b8c085c544859e24a7f"><code>add_down</code></a></td><td><a href="cuda-battery/namespacebattery.html#a6d6340503a20225d569990c0044519bb"><code>sub_up</code></a></td><td><a href="cuda-battery/namespacebattery.html#a32ff1fe9f8d2eac8fd7a2d08b0110461"><code>sub_down</code></a></td></tr>
<tr><td></td><td><a href="cuda-battery/namespacebattery.html#ae3edf2725aaea683aff7a100733b26f2"><code>mul_up</code></a></td><td><a href="cuda-battery/namespacebattery.html#a6dd3e5546b5286d98cb29c7560542759"><code>mul_down</code></a></td><td><a href="cuda-battery/namespacebattery.html#a3ce4b4df0f80c5b19c7d3b401464f309"><code>div_up</code></a></td><td><a href="cuda-battery/namespacebattery.html#ac253a56f7fa54ade8f2eb762d3b317f9"><code>div_down</code></a></td></tr>
<tr><td><a href="cuda-battery/memory_8hpp.html">Memory</a></td><td><a href="cuda-battery/namespacebattery.html#a09111ca968cc4d8defa60555963dd052"><code>local_memory</code></a></td><td><a href="cuda-battery/namespacebattery.html#a22ff3da8ce553868de9c2b8fe604fe3c"><code>read_only_memory</code></a></td><td><a href="cuda-battery/classbattery_1_1atomic__memory.html"><code>atomic_memory</code></a></td><td></td></tr>
<tr><td></td><td><a href="cuda-battery/classbattery_1_1atomic__memory__scoped.html"><code>atomic_scoped_memory</code></a></td><td><a href="cuda-battery/namespacebattery.html#afb485d8f961537d1ca590f78d16ac1c4"><code>atomic_memory_block</code></a></td><td><a href="cuda-battery/namespacebattery.html#a2af42ce969d94b6b8bb1ed9a94b9cf49"><code>atomic_memory_grid</code></a></td><td></td></tr>
</tbody></table>
<h2 id="highlights"><a class="header" href="#highlights">Highlights</a></h2>
<ul>
<li><a href="CUDA-Battery.html#transferring-data-from-the-cpu-to-the-gpu">How to transfer data from the CPU to the GPU?</a></li>
<li><a href="CUDA-Battery.html#cmake-cuda-project">How to create a CMake project for CUDA project?</a></li>
<li><a href="CUDA-Battery.html#block-local-memory">How to allocate a vector shared by all threads of a block inside a kernel?</a></li>
<li><a href="CUDA-Battery.html#grid-local-memory">How to allocate a vector shared by all blocks inside a kernel?</a></li>
<li><a href="CUDA-Battery.html#avoiding-obscure-cuda-runtime-errors">CUDA runtime error an illegal memory access was encountered</a></li>
<li><a href="CUDA-Battery.html#shared-memory-allocator">How to allocate a vector in shared memory?</a></li>
</ul>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>The main objective of this library is to make it easier to develop software fully executing on the GPU.
This is in contrast to applications where the CPU is the &quot;director&quot;, and the GPU is only there to execute small kernels performing specific parallel tasks.
We want to have the whole computation on the GPU when possible.
Why you asked?
Because it is more efficient as we avoid round-trips between CPU and GPU; although calling a kernel is very fast, the memory transfers are still costly.
In the research project <a href="https://github.com/lattice-land">lattice-land</a>, we also explore pure GPU computation as a new paradigm where many small functions collaborate in parallel to reach a common goal.</p>
<p>Developing systems in CUDA is a daunting task because the CUDA API is reminiscent of C programming, and it lacks modern C++ idioms and data structures.
For instance, memory allocations and deallocations are managed by malloc/free variants, arrays are pointers and we do not have access to the C++ STL.
Moreover, due to the specificities of the CUDA API, it is usually difficult to program a single function working on both the CPU and the GPU.
Although it is rarely possible to have the exact same algorithm on both CPU and GPU, there are often algorithmic components that are shared.
Having them both on CPU and GPU can help to verify the parallel version produces the same result as the sequential version, and it also allows us to benchmark CPU vs GPU.</p>
<p>The <a href="https://github.com/lattice-land/cuda-battery">cuda-battery</a> library reimplements basic data structures from the STL in a way such that the same code runs on both CPU and GPU.
One of the only technical differences between CPU and GPU is memory allocation.
To overcome this difference, we provide various allocators that allocate memory in the CPU memory, unified memory, and GPU global and shared memory.
The idea is then to parametrize your classes (through a C++ template) with an allocator type and, depending on whether you run your algorithm on the CPU or GPU, to instantiate it with a suited allocator type.
Among the supported data structures, we have <code>vector</code>, <code>string</code>, <code>variant</code>, <code>tuple</code>, <code>unique_ptr</code>, <code>shared_ptr</code>, a variant of <code>bitset</code> and many utility functions.
In addition to the STL, we provide an abstraction of the memory accesses to enable non-atomic (sequential) and atomic (parallel) read and write operations.
The memory abstraction is necessary when writing code working on both GPU and CPU, but also when writing GPU parallel code that can work in global and shared memory, and at the thread, block, grid and multi-grid level.</p>
<p>This library aims to be generic and usable in different kind of projects, but when a design decision needs to be made, it will be influenced by the needs of the project <a href="https://github.com/lattice-land">lattice-land</a>.</p>
<p>Everything is under the namespace <code>battery::</code>.
The Doxygen documentation is available <a href="cuda-battery/index.html">here</a>.
Due to lack of time, these are often partial implementation of their STL counterpart, and there are sometimes (documented) differences.</p>
<p>NVIDIA is developing <a href="https://nvidia.github.io/libcudacxx/">libcudacxx</a>, a version of the standard C++ library compatible with GPU, and with extensions specific to CUDA.
<em>cuda-battery</em> can be seen as an extension of <em>libcudacxx</em>, and we intend to remove the data structures proposed here as soon as they become available in <em>libcudacxx</em>.
Another well-known library is <a href="https://github.com/NVIDIA/thrust">thrust</a>, but it does not share the same goal since it hides the parallel computation inside the data structure, e.g. <code>reduce</code> on a <code>vector</code> is automatically parallelized on the GPU.</p>
<p>Note that this tutorial does not introduce basic CUDA concepts: it is for (possibly beginners) CUDA programmers who want to simplify their code.
For an introduction to CUDA, you can first refer to <a href="https://ulhpc-tutorials.readthedocs.io/en/latest/gpu/cuda2023/">this tutorial</a>.
Finally, the full code of all examples given here is available in a <a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo">demo project</a>.</p>
<h2 id="transferring-data-from-the-cpu-to-the-gpu"><a class="header" href="#transferring-data-from-the-cpu-to-the-gpu">Transferring data from the CPU to the GPU</a></h2>
<p>(<a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/src/demo.cpp">code of this section</a>)</p>
<p>One of the first tasks a CUDA programmer is facing is to transfer data from the CPU to the GPU.
Using this library, it is very straightforward, and it is always the same scheme.
To illustrate the concepts of this library, we implement a <code>map(v, f)</code> function which applies a function <code>f</code> to all elements of the sequence <code>v</code> in-place.</p>
<p>We firstly need to transfer the data <code>v</code> to the GPU using managed memory, and it is as simple as that:</p>
<pre><code class="language-c++">int main() {
  std::vector&lt;int&gt; v(1000, 50);
  battery::vector&lt;int, battery::managed_allocator&gt; gpu_v(v);
  return 0;
}
</code></pre>
<p>Now we must pass <code>gpu_v</code> to a CUDA kernel.
However, there is a slight technical issue due to the weird parameters passing semantics of CUDA kernel (function with <code>__global__</code>): we must pass primitive types or pointers to the functions.
Indeed, trying to pass an object by reference or copy <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters">is undefined behavior</a>.
This is only a restriction on <code>__global__</code> functions, and once inside the kernel, passing by reference or copy works well (when calling <code>__device__</code> functions).
You could simply pass the data as <code>kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(gpu_v.data(), gpu_v.size())</code>, but you lose all the advantages of vector when programming in the kernel.
The solution is to wrap <code>gpu_v</code> inside a pointer, which we can still do the C++ way using <code>unique_ptr</code>:</p>
<pre><code class="language-c++">using mvector = battery::vector&lt;int, battery::managed_allocator&gt;;

__global__ void map_kernel(mvector* v_ptr) {
  mvector&amp; v = *v_ptr;
  // ... Compute on `v` in parallel.
}

void map(std::vector&lt;int&gt;&amp; v) {
  auto gpu_v = battery::make_unique&lt;mvector, battery::managed_allocator&gt;(v);
  map_kernel&lt;&lt;&lt;256, 256&gt;&gt;&gt;(gpu_v.get());
  // Transfering the new data to the initial vector.
  for(int i = 0; i &lt; v.size(); ++i) {
    v[i] = (*gpu_v)[i];
  }
}
</code></pre>
<p>Due to the <a href="https://en.cppreference.com/w/cpp/language/raii">C++ RAII idiom</a>, the managed memory of both the GPU <code>unique_ptr</code> and <code>mvector</code> is automatically freed when leaving the scope of the function.
Importantly, the memory allocated on the CPU must be freed by the CPU, even if it is accessible by the GPU, and vice-versa.
But you should not encounter this issue if you use this idiom to pass data from the CPU to the GPU.</p>
<h2 id="cmake-cuda-project"><a class="header" href="#cmake-cuda-project">CMake CUDA Project</a></h2>
<p>(<a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/CMakeLists.txt">code of this section</a>)</p>
<p>In order to compile and test the code presented above, you will need to add the headers of this library to <code>nvcc</code>.</p>
<pre><code class="language-bash">nvcc -I cuda-battery/include demo.cu
</code></pre>
<p>We prefer to delegate the management of dependencies to <a href="https://cmake.org/">CMake</a>, a build automation tool.
However, creating a CMake project for hybrid CPU/GPU code is not an easy task, and we provide a demonstration CMake project in <a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo">cuda-battery/demo</a>.
You can start your own project by copying this folder and modifying the name of the project inside the file <code>CMakeLists.txt</code>.
To compile and run this demo project, you can write:</p>
<pre><code>cd cuda-battery/demo
mkdir -p build/gpu-debug
cmake -DCMAKE_BUILD_TYPE=Debug -Bbuild/gpu-debug
cmake --build build/gpu-debug
./build/gpu-debug/demo
</code></pre>
<p>It compiles the demo project in debug mode using the GPU compiler (<code>nvcc</code>), along with the unit tests (using the Google testing framework GTest).
You can also compile it in release mode by simply changing <code>debug</code> to <code>release</code> in the previous commands.
To run the tests, the command <code>ctest</code> can be used as follows:</p>
<pre><code>ctest --test-dir build/gpu-debug/
</code></pre>
<p>Among the characteristics of this project:</p>
<ul>
<li>Files have the <code>.cpp</code> extension instead of the <code>.cu</code> extension.</li>
<li>It compiles code for the native GPU architecture by default (so for the GPU of the computer you are compiling your code on).
This can easily be changed if you are cross-compiling by defining the flag <code>CMAKE_CUDA_ARCHITECTURES</code> at the configuration stage:</li>
</ul>
<pre><code>cmake -DCMAKE_CUDA_ARCHITECTURES=70 -DCMAKE_BUILD_TYPE=Release -Bbuild/gpu-release
</code></pre>
<ul>
<li>Several useful options inherited from cuda-battery (enabling C++20 and constexpr extension).</li>
<li>A testing framework where you can write your CPU tests using Google Test framework (see <code>demo/tests/demo_test.cpp</code>) and your hand-made GPU tests (see <code>demo/tests/demo_test_gpu.cpp</code>).</li>
<li>Moreover, when testing GPU code, we verify there is no memory leaks or some data races (using <code>compute-sanitizer</code>).</li>
</ul>
<p>We have documented the <a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/CMakeLists.txt">CMakeLists.txt</a> so you can adjust it to your own project.</p>
<h2 id="in-kernel-memory-allocation"><a class="header" href="#in-kernel-memory-allocation">In-Kernel Memory Allocation</a></h2>
<p>(<a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/src/inkernel_allocation.cpp">code of this section</a>)</p>
<p>The typical way to allocate data structures shared by all threads or blocks in a CUDA kernel is to get all the memory allocations done before the kernel starts.
However, when developing a larger CUDA kernel, it is frequent to rely on intermediate structures that are only temporary and are deleted before the end of the kernel.
It might be difficult, or at best inconvenient, to allocate everything before the start of the kernel.
Moreover, to follow good software engineering practice, these temporary structures should be hidden from the user of the kernel.</p>
<h3 id="thread-local-memory"><a class="header" href="#thread-local-memory">Thread-local Memory</a></h3>
<p>Thread-local memory is a chunk of memory that is only accessed by a single thread.
This is the default allocation mode when declaring a variable in CUDA kernel, hence it does not need any support from this library.</p>
<h3 id="block-local-memory"><a class="header" href="#block-local-memory">Block-local Memory</a></h3>
<p>Block-local memory is a chunk of memory that is only accessed by the threads of a single block.
To avoid any ambiguity: &quot;block-local memory&quot; is conceptual and might reside in global memory; it is not necessarily in shared memory.</p>
<p>A simple use-case is when blocks need to work on different copies of an original array.
Suppose <code>*v_ptr</code> is an array shared by all blocks.
In the following kernel, we show how to use <code>battery::make_unique_block</code> to copy <code>*v_ptr</code> into a block-local vector <code>v_block</code>.</p>
<pre><code class="language-c++">using gvector = battery::vector&lt;int, battery::global_allocator&gt;;
__global__ void block_vector_copy(mvector* v_ptr) {
  battery::unique_ptr&lt;gvector, battery::global_allocator&gt; v_block_ptr;
  gvector&amp; v_block = battery::make_unique_block(v_block_ptr, *v_ptr);
  // Now each block has its own local copy of the vector `*v_ptr`.
  // ...
  // We must synchronize the threads at the end, in case the thread holding the pointer in `unique_ptr` terminates before the other.
  cooperative_groups::this_thread_block().sync(); // Alternatively, `__syncthreads();`
}
</code></pre>
<p>Without this facility, we would need to initialize <code>n</code> copies of the vector in the host code and pass them as parameters to the kernel.
Finally, the function <code>make_unique_block</code> synchronizes all threads of the current block before returning, therefore <code>v_block</code> is directly usable by all threads.
Before you use this technique, keep reading because you might need to increase the size of the heap and stack.</p>
<h3 id="avoiding-obscure-cuda-runtime-errors"><a class="header" href="#avoiding-obscure-cuda-runtime-errors">Avoiding Obscure CUDA Runtime Errors</a></h3>
<p>Developing an entire system within a single kernel can easily lead to CUDA runtime error due to overflow of the allowed heap and stack memory.
The heap memory is by-default limited to 8 MB for allocations taking place in the kernel.
If you allocate more than 8 MB, which is not very difficult, you will run into an error of the style &quot;CUDA runtime error an illegal memory access was encountered&quot;.
In that case, you must increase the size of the heap, and this can be done as follows:</p>
<pre><code class="language-c++">// Multiply by 10 the default value, so now we have 80MB.
void increase_heap_size() {
  size_t max_heap_size;
  cudaDeviceGetLimit(&amp;max_heap_size, cudaLimitMallocHeapSize);
  CUDAE(cudaDeviceSetLimit(cudaLimitMallocHeapSize, max_heap_size*10));
  cudaDeviceGetLimit(&amp;max_heap_size, cudaLimitMallocHeapSize);
  printf(&quot;%%GPU_max_heap_size=%zu (%zuMB)\n&quot;, max_heap_size, max_heap_size/1000/1000);
}

int main() {
  increase_heap_size();
  auto vptr = battery::make_unique&lt;mvector, battery::managed_allocator&gt;(100000, 42);
  auto ptr = vptr.get();

  block_vector_copy&lt;&lt;&lt;256, 256&gt;&gt;&gt;(ptr);
  CUDAEX(cudaDeviceSynchronize());
}
</code></pre>
<p>For the stack, which is allocated per-thread, the problem can quickly arrive if you have many function calls and local variables.
In that case you can increase the size of the stack as follows:</p>
<pre><code class="language-c++">void increase_stack_size() {
  size_t max_stack_size = 1024;
  CUDAE(cudaDeviceSetLimit(cudaLimitStackSize, max_stack_size*10));
  cudaDeviceGetLimit(&amp;max_stack_size, cudaLimitStackSize);
  printf(&quot;%%GPU_max_stack_size=%zu (%zuKB)\n&quot;, max_stack_size, max_stack_size/1000);
}
</code></pre>
<p>For information, the stack frames are stored in global memory, but the compiler will try its best to place them in the registers and caches when possible.</p>
<h3 id="grid-local-memory"><a class="header" href="#grid-local-memory">Grid-local Memory</a></h3>
<p>Similarly to the previous section, we sometimes wish to initialize, inside the kernel, data that is shared by all blocks.
Once again, we suppose to have an original array <code>*v_ptr</code> that we wish to copy, but per-grid and not per-block.</p>
<pre><code class="language-c++">__global__ void grid_vector_copy(mvector* v_ptr) {
  battery::unique_ptr&lt;gvector, battery::global_allocator&gt; v_copy_ptr;
  gvector&amp; v_copy = battery::make_unique_grid(v_copy_ptr, *v_ptr);
  // `v_copy` is now accessible by all blocks.
  // ...
  // Same as with block-local memory, we want to guard against destructing the pointer too early.
  cooperative_groups::this_grid().sync();
}
</code></pre>
<p>To synchronize among threads, both <code>make_unique_block</code> and <code>make_unique_grid</code> rely on <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups">cooperative groups</a>.
In the case of <code>make_unique_grid</code>, CUDA requires the kernel to be launched with a <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#grid-synchronization">different syntax</a>:</p>
<pre><code class="language-c++">int main() {
  increase_heap_size();
  auto vptr = battery::make_unique&lt;mvector, battery::managed_allocator&gt;(100000, 42);
  auto ptr = vptr.get();
  void *kernelArgs[] = { &amp;ptr }; // be careful, we need to take the address of the parameter we wish to pass.
  dim3 dimBlock(256, 1, 1);
  dim3 dimGrid(256, 1, 1);
  cudaLaunchCooperativeKernel((void*)grid_vector_copy, dimGrid, dimBlock, kernelArgs);
  CUDAE(cudaDeviceSynchronize());
  return 0;
}
</code></pre>
<p>I am not sure why the syntax is different, but since it is a fairly recent feature, it might be improved in future releases.</p>
<h3 id="multi-grid-memory"><a class="header" href="#multi-grid-memory">Multi-grid Memory</a></h3>
<p>For now, we do not support multi-grid memory allocation.</p>
<h2 id="shared-memory-allocator"><a class="header" href="#shared-memory-allocator">Shared Memory Allocator</a></h2>
<p>(<a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/src/demo.cpp">code of this section</a>)</p>
<p>We show how to use shared memory using a memory pool allocator.</p>
<pre><code class="language-c++">using pvector = battery::vector&lt;int, battery::pool_allocator&gt;;

__global__ void map_kernel_shared(mvector* v_ptr, size_t shared_mem_capacity) {
  // I. Create a pool of shared memory.
  extern __shared__ unsigned char shared_mem[];
  battery::unique_ptr&lt;battery::pool_allocator, battery::global_allocator&gt; pool_ptr;
  // /!\ We must take a reference to the pool_allocator to avoid copying it, because its copy-constructor is not thread-safe! (It can only be used by one thread at a time).
  battery::pool_allocator&amp; shared_mem_pool = battery::make_unique_block(pool_ptr, static_cast&lt;unsigned char*&gt;(shared_mem), shared_mem_capacity);

  // II. Transfer from global memory to shared memory.
  battery::unique_ptr&lt;pvector, battery::global_allocator&gt; shared_vector;
  size_t chunk_size = chunk_size_per_block(*v_ptr, gridDim.x);
  auto span = make_safe_span(*v_ptr, chunk_size * blockIdx.x, chunk_size);
  pvector&amp; v = battery::make_unique_block(shared_vector, span.data(), span.size(), shared_mem_pool);

  // III. Run the algorithm on the shared memory.
  block_par_map(v, [](int x){ return x * 2; }, blockDim.x, threadIdx.x);
  __syncthreads();

  // IV. Transfer back from shared memory to global memory.
  for(int i = threadIdx.x; i &lt; v.size(); i += blockDim.x) {
    (*v_ptr)[chunk_size * blockIdx.x + i] = v[i];
  }
}
</code></pre>
<p>We initialize one pool allocator per-block using the same technique as shown before.
However, we must be careful to take the <code>pool_allocator</code> by reference because its copy-constructor is not thread-safe; similarly to <code>shared_ptr</code> it maintains a shared counter to its memory pool.
Since memory allocation is done by only one thread anyway, it does not make sense to have multiple copies of this allocator.</p>
<p>The next step is to transfer the vector to the shared memory.
Each block works on a chunk of the initial array.
Therefore, we do not want to move the whole array in shared memory, but only the part of interest for that block.
To achieve that, we compute the size of the chunk using <code>chunk_size_per_block</code> and create a view of the vector covering only that chunks.
We use <a href="https://en.cppreference.com/w/cpp/container/span">std::span</a> to model the view.
Note that we can use the STL <code>std::span</code> because all its methods are <code>constexpr</code>, and we have the NVCC flag <code>--expt-relaxed-constexpr</code> inherited from cuda-battery.
We then copy this span into the vector <code>v</code> which is in shared memory.</p>
<p>The algorithm <code>block_par_map</code> is then called on <code>v</code> for each block.
The last step is to transfer the memory from the shared memory to the global memory.
After each block finished computing, the <code>map</code> function has been applied to each element of the initial vector.</p>
<h2 id="a-word-of-caution-on-shared-state-parallelism"><a class="header" href="#a-word-of-caution-on-shared-state-parallelism">A Word of Caution on Shared-state Parallelism</a></h2>
<p>Developing a parallel algorithm is relatively easy as long as threads read and write independent memory chunks.
Such an algorithm is referred as <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a>.
As a rule of thumb, it is best if you can stay within this paradigm.
For instance using map-reduce operations, where all threads work on separate memory cells during the map operation, and then a single thread performs a reduction of all temporary results obtained.
Sometimes this is not possible, or you want to get to the next level to get better performance.
Beware, designing <em>correct and efficient</em> parallel algorithms with threads communicating in a shared memory system is a whole new world.
We cannot even scratch the surface here, and this library does not provide much support for it.
Nevertheless, it is possible to write such algorithms in CUDA using <a href="https://nvidia.github.io/libcudacxx/extended_api/synchronization_primitives.html">atomics and synchronization primitives</a>.
To use them well, I would advise to take a class (or <a href="https://arxiv.org/abs/1701.00854">read a book</a>) on parallel programming and memory consistency models.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parallel-minimum-algorithm"><a class="header" href="#parallel-minimum-algorithm">Parallel Minimum Algorithm</a></h1>
<p>We present a simple algorithm computing the minimum element of an array in parallel on GPU.
We introduce the main concepts behind this framework, and give a first grasp to use the library.</p>
<h2 id="an-array-of-element-on-gpu"><a class="header" href="#an-array-of-element-on-gpu">An array of element on GPU</a></h2>
<p>The first thing to do is to create an array of numbers to be passed to a GPU kernel.
As easy as it seems, there is already a small pitfall not to fall in.
Thanks to our library <a href="CUDA-Battery.html">cuda-battery</a>, we can use a <code>vector&lt;int&gt;</code> similarly to what we use on CPU.
Surprisingly, such a simple <code>vector</code> class is not yet available on GPU, although they are working on a copy of the STL called <a href="https://nvidia.github.io/libcudacxx/">libcudacxx</a> working on CUDA GPU.
As soon as <code>vector</code> is available there, we will use the standard one, meanwhile, let's use the one from cuda-battery.
One of the main distinction between CPU and GPU programming is the memory.
On the CPU side, we allocate using <code>new</code> or <code>malloc</code>, but on GPU you should allocate using the more specific <code>cudaMalloc</code>.
Although it does not seem like a big deal, it makes developing code working on both CPU and GPU a bit harder.
We rely on the <em>allocators</em> provided by cuda-battery, and basically all the code is templated by a <code>class Allocator</code>.
In the standard library, the class <code>vector</code> is parametrized by an allocator, and so is the <code>battery::vector</code> version.
Although most of us never use it, it is actually not very difficult.
The safest allocator to use is <code>battery::ManagedAllocator</code> which uses the managed memory of CUDA, which is a memory automatically accessible from the CPU and GPU, and the data migrates automatically between the two worlds.</p>
<pre><code class="language-c++">#include &quot;vector.hpp&quot; // From cuda-battery.
#include &quot;allocator.hpp&quot; // From cuda-battery.

int main() {
  battery::vector&lt;int, battery::ManagedAllocator&gt; v{5, 1, 2, 3};
}
</code></pre>
<p>To make the type shorter, let's use the following alias:</p>
<pre><code class="language-c++">using namespace battery; // let's make things shorter for this tutorial.
using ivector = vector&lt;int, ManagedAllocator&gt;;
</code></pre>
<p>A common use-case is to declare and initialize an array on the CPU and then pass it to the GPU.
However, there is a catch...
None of the following kernel declaration will work as we would like to:</p>
<pre><code class="language-c++">__global__ void min_kernel(ivector&amp; v);
__global__ void min_kernel(const ivector&amp; v);
__global__ void min_kernel(ivector v);
</code></pre>
<p>The two first are not working because the address of <code>v</code> in the main is an address on the CPU side, and of course it does not work when we try to access it on the GPU, so it will segfault.
The third version works but it performs an unwanted copy of the array.
After all, we used a managed allocator to avoid copying the array to the GPU once initialized and declared on the CPU.
There is not a hundred ways to achieve what we want: we need to pass the data through a pointer allocated on the GPU.
Since we are coding in &quot;modern C++&quot;, we will avoid manipulating raw pointers, and instead will rely on <code>shared_ptr</code>.
Once again, a CUDA-friendly version of <code>shared_ptr</code> is provided by cuda-battery.</p>
<pre><code class="language-c++">#include &quot;shared_ptr.hpp&quot;
#include &quot;vector.hpp&quot;
#include &quot;allocator.hpp&quot;

using namespace battery;
using ivector = vector&lt;int, ManagedAllocator&gt;;
using svector = shared_ptr&lt;ivector, ManagedAllocator&gt;;

__global__ void min_kernel(svector v);

int main() {
  svector v = make_shared&lt;ivector, ManagedAllocator&gt;(ivector{5, 1, 2, 3});
}
</code></pre>
<p>Now we can pass an array to our GPU kernel, we can start working on the minimum algorithm.
The result of the algorithm is retrieved using an integer pointer, because CUDA kernels cannot return value.
The rule of thumb is to use <code>shared_ptr&lt;T, ManagedMemory&gt;</code> to pass any argument of type <code>T</code> to a kernel, unless it is a primitive constant.
If you do that, we do not even need to worry about deallocation.
But using <code>shared_ptr</code> for parameters passing is just necessary for the kernel, afterwards, you can use reference parameter passing as usual (see <code>minimum</code> below).</p>
<pre><code class="language-c++">CUDA int minimum(const ivector&amp; v) {
  int result = std::numeric_limits&lt;int&gt;::max();
  for (int i = 0; i &lt; v.size(); ++i) {
    result = min(result, v[i]);
  }
  return result;
}

__global__ void min_kernel(svector v, shared_ptr&lt;int, ManagedAllocator&gt; result) {
  *result = minimum(*v);
}

int main() {
  svector v = make_shared&lt;ivector, ManagedAllocator&gt;(ivector{5, 1, 2, 3});
  shared_ptr&lt;int, ManagedAllocator&gt; result =
    make_shared&lt;int, ManagedAllocator&gt;(std::numeric_limits&lt;int&gt;::max());
  min_kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;(v, result);
  CUDIE(cudaDeviceSynchronize());
  printf(&quot;minimum: %d\n&quot;, *result);
}
</code></pre>
<h2 id="parallelizing-the-minimum-algorithm"><a class="header" href="#parallelizing-the-minimum-algorithm">Parallelizing the minimum algorithm</a></h2>
<p>The code presented above is still single-threaded, although it runs on the GPU.
A common approach to parallelize <code>minimum</code> is to divide the array into <code>N</code> parts, where <code>N</code> is the number of threads available.
Each thread then computes a local minimum, and a single thread eventually aggregates the results.</p>
<div style="break-before: page; page-break-before: always;"></div>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
