<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Lattice Land Book</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Lattice Land</a></li><li class="chapter-item expanded affix "><li class="part-title">CUDA Battery Library</li><li class="chapter-item expanded "><a href="1-cuda-battery.html"><strong aria-hidden="true">2.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="2-cuda-battery.html"><strong aria-hidden="true">3.</strong> Data from CPU to GPU</a></li><li class="chapter-item expanded "><a href="3-cuda-battery.html"><strong aria-hidden="true">4.</strong> CMake Project</a></li><li class="chapter-item expanded "><a href="4-cuda-battery.html"><strong aria-hidden="true">5.</strong> In-Kernel Allocation</a></li><li class="chapter-item expanded "><a href="5-cuda-battery.html"><strong aria-hidden="true">6.</strong> Shared Memory Allocator</a></li><li class="chapter-item expanded "><a href="6-cuda-battery.html"><strong aria-hidden="true">7.</strong> Caution</a></li><li class="chapter-item expanded affix "><li class="part-title">Parallel Lattice Programming</li><li class="chapter-item expanded affix "><li class="part-title">Abstract Constraint Reasoning</li><li class="chapter-item expanded affix "><li class="part-title">Parallel Abstract Constraint Reasoning</li><li class="chapter-item expanded affix "><li class="part-title">Turbo Technical Journal</li><li class="chapter-item expanded "><a href="1-turbo.html"><strong aria-hidden="true">8.</strong> Quest for Efficiency</a></li><li class="chapter-item expanded "><a href="2-turbo.html"><strong aria-hidden="true">9.</strong> Design Rational</a></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">Lattice Land Book</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="lattice-land"><a class="header" href="#lattice-land">Lattice Land</a></h1>
<p>Hello! You just arrived in lattice land!
Lattice land is a collection of C++ libraries compatible with NVIDIA CUDA framework.
Most of the libraries implement <em>abstract domains for constraint reasoning</em> a new kind of data structure based on <a href="https://en.wikipedia.org/wiki/Abstract_interpretation">abstract interpretation</a>, <a href="https://en.wikipedia.org/wiki/Lattice_(order)">lattice theory</a> and <a href="https://en.wikipedia.org/wiki/Constraint_satisfaction">constraint reasoning</a>.
This is a book presenting our research project, the involved research papers and the code documentation.</p>
<p>This project is available on <a href="https://github.com/lattice-land">github</a>.</p>
<h2 id="api-documentation"><a class="header" href="#api-documentation">API Documentation</a></h2>
<ul>
<li><a href="https://lattice-land.github.io/cuda-battery">cuda-battery</a>: Memory allocators, vector, utilities and more which run on both CPU and GPU (CUDA).
See also the chapter <a href="1-cuda-battery.html">CUDA-Battery Library</a>.</li>
<li><a href="https://lattice-land.github.io/lala-core">lala-core</a>: Core definitions of the formula AST and abstract domain.</li>
<li><a href="https://lattice-land.github.io/lala-parsing">lala-parsing</a>: Utilities to parse combinatorial formats include flatzinc and XCSP3.</li>
<li><a href="https://lattice-land.github.io/lala-pc">lala-pc</a>: <em>Propagator completion abstract domain</em> representing a collection of refinement functions. It implements propagators, an essential component of constraint solver.</li>
<li><a href="https://lattice-land.github.io/lala-power">lala-power</a>: Abstract domains representing disjunctive collections of information. It includes the search tree and branch-and-bound.</li>
<li><a href="https://lattice-land.github.io/turbo">turbo</a>: Abstract constraint solver building on all other libraries.</li>
<li><a href="https://github.com/lattice-land/lattice-land.github.io">lattice-land.github.io</a>: The repository hosting this book and the libraries documentation.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>The main objective of the CUDA battery library is to make it easier to develop software fully executing on the GPU.
This is in contrast to applications where the CPU is the &quot;director&quot;, and the GPU is only there to execute small kernels performing specific parallel tasks.
We want to have the whole computation on the GPU when possible.
Why you asked?
Because it is more efficient as we avoid round-trips between CPU and GPU; although calling a kernel is very fast, the memory transfers are still costly.
In the research project <a href="https://github.com/lattice-land">lattice-land</a>, we also explore pure GPU computation as a new paradigm where many small functions collaborate in parallel to reach a common goal.</p>
<p>Developing systems in CUDA is a daunting task because the CUDA API is reminiscent of C programming, and it lacks modern C++ idioms and data structures.
For instance, memory allocations and deallocations are managed by malloc/free variants, arrays are pointers and we do not have access to the C++ STL.
Moreover, due to the specificities of the CUDA API, it is usually difficult to program a single function working on both the CPU and the GPU.
Although it is rarely possible to have the exact same algorithm on both CPU and GPU, there are often algorithmic components that are shared.
Having them both on CPU and GPU can help to verify the parallel version produces the same result as the sequential version, and it also allows us to benchmark CPU vs GPU.</p>
<p>The <a href="https://github.com/lattice-land/cuda-battery">cuda-battery</a> library reimplements basic data structures from the STL in a way such that the same code runs on both CPU and GPU.
One of the only technical differences between CPU and GPU is memory allocation.
To overcome this difference, we provide various allocators that allocate memory in the CPU memory, unified memory, and GPU global and shared memory.
The idea is then to parametrize your classes (through a C++ template) with an allocator type and, depending on whether you run your algorithm on the CPU or GPU, to instantiate it with a suited allocator type.
Among the supported data structures, we have <code>vector</code>, <code>string</code>, <code>variant</code>, <code>tuple</code>, <code>unique_ptr</code>, <code>shared_ptr</code>, a variant of <code>bitset</code> and many utility functions.
In addition to the STL, we provide an abstraction of the memory accesses to enable non-atomic (sequential) and atomic (parallel) read and write operations.
The memory abstraction is necessary when writing code working on both GPU and CPU, but also when writing GPU parallel code that can work in global and shared memory, and at the thread, block, grid and multi-grid level.</p>
<p>This library aims to be generic and usable in different kind of projects, but when a design decision needs to be made, it will be influenced by the needs of the project <a href="https://github.com/lattice-land">lattice-land</a>.</p>
<p>Everything is under the namespace <code>battery::</code>.
The Doxygen documentation is available <a href="cuda-battery/index.html">here</a>.
Due to lack of time, these are often partial implementation of their STL counterpart, and there are sometimes (documented) differences.</p>
<p>NVIDIA is developing <a href="https://nvidia.github.io/libcudacxx/">libcudacxx</a>, a version of the standard C++ library compatible with GPU, and with extensions specific to CUDA.
<em>cuda-battery</em> can be seen as an extension of <em>libcudacxx</em>, and we intend to remove the data structures proposed here as soon as they become available in <em>libcudacxx</em>.
Another well-known library is <a href="https://github.com/NVIDIA/thrust">thrust</a>, but it does not share the same goal since it hides the parallel computation inside the data structure, e.g. <code>reduce</code> on a <code>vector</code> is automatically parallelized on the GPU.</p>
<p>Note that this tutorial does not introduce basic CUDA concepts: it is for (possibly beginners) CUDA programmers who want to simplify their code.
For an introduction to CUDA, you can first refer to <a href="https://ulhpc-tutorials.readthedocs.io/en/latest/gpu/cuda2023/">this tutorial</a>.
Finally, the full code of all examples given here is available in a <a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo">demo project</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transferring-data-from-the-cpu-to-the-gpu"><a class="header" href="#transferring-data-from-the-cpu-to-the-gpu">Transferring data from the CPU to the GPU</a></h1>
<p>(<a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/src/demo.cpp">code of this section</a>)</p>
<p>One of the first tasks a CUDA programmer is facing is to transfer data from the CPU to the GPU.
Using this library, it is very straightforward, and it is always the same scheme.
To illustrate the concepts of this library, we implement a <code>map(v, f)</code> function which applies a function <code>f</code> to all elements of the sequence <code>v</code> in-place.</p>
<p>We firstly need to transfer the data <code>v</code> to the GPU using managed memory, and it is as simple as that:</p>
<pre><code class="language-c++">int main() {
  std::vector&lt;int&gt; v(1000, 50);
  battery::vector&lt;int, battery::managed_allocator&gt; gpu_v(v);
  return 0;
}
</code></pre>
<p>Now we must pass <code>gpu_v</code> to a CUDA kernel.
However, there is a slight technical issue due to the weird parameters passing semantics of CUDA kernel (<code>__global__</code> function): we must pass primitive types or pointers to the functions.
Indeed, trying to pass an object by reference or copy <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters">is undefined behavior</a>.
This is only a restriction on <code>__global__</code> functions, and once inside the kernel, passing by reference or copy works well (when calling <code>__device__</code> functions).
You could simply pass the data as <code>kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(gpu_v.data(), gpu_v.size())</code>, but you lose all the advantages of vector when programming in the kernel.
The solution is to wrap <code>gpu_v</code> inside a pointer, which we can still do the C++ way using <code>unique_ptr</code>:</p>
<pre><code class="language-c++">using mvector = battery::vector&lt;int, battery::managed_allocator&gt;;

__global__ void map_kernel(mvector* v_ptr) {
  mvector&amp; v = *v_ptr;
  // ... Compute on `v` in parallel.
}

void map(std::vector&lt;int&gt;&amp; v) {
  auto gpu_v = battery::make_unique&lt;mvector, battery::managed_allocator&gt;(v);
  map_kernel&lt;&lt;&lt;256, 256&gt;&gt;&gt;(gpu_v.get());
  CUDAEX(cudaDeviceSynchronize());
  // Transfering the new data to the initial vector.
  for(int i = 0; i &lt; v.size(); ++i) {
    v[i] = (*gpu_v)[i];
  }
}
</code></pre>
<p>Due to the <a href="https://en.cppreference.com/w/cpp/language/raii">C++ RAII idiom</a>, the managed memory of both the GPU <code>unique_ptr</code> and <code>mvector</code> is automatically freed when leaving the scope of the function.
Importantly, the memory allocated on the CPU must be freed by the CPU, even if it is accessible by the GPU, and vice-versa.
But you should not encounter this issue if you use this idiom to pass data from the CPU to the GPU.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cmake-cuda-project"><a class="header" href="#cmake-cuda-project">CMake CUDA Project</a></h1>
<p>(<a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/CMakeLists.txt">code of this section</a>)</p>
<p>In order to compile and test the code presented above, you will need to add the headers of this library to <code>nvcc</code>.</p>
<pre><code class="language-bash">nvcc -I cuda-battery/include demo.cu
</code></pre>
<p>We prefer to delegate the management of dependencies to <a href="https://cmake.org/">CMake</a>, a build automation tool.
However, creating a CMake project for hybrid CPU/GPU code is not an easy task, and we provide a demonstration CMake project in <a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo">cuda-battery/demo</a>.
You can start your own project by copying the <code>demo</code> folder and modifying the name of the project inside the file <code>CMakeLists.txt</code>.
To compile and run this project, you can write:</p>
<pre><code>cd cuda-battery/demo
mkdir -p build/gpu-debug
cmake -DCMAKE_BUILD_TYPE=Debug -Bbuild/gpu-debug
cmake --build build/gpu-debug
./build/gpu-debug/demo
</code></pre>
<p>It compiles the demo project in debug mode using the GPU compiler (<code>nvcc</code>), along with the unit tests (using the Google testing framework GTest).
You can also compile it in release mode by simply changing <code>debug</code> to <code>release</code> in the previous commands.
To run the tests, the command <code>ctest</code> can be used as follows:</p>
<pre><code>ctest --test-dir build/gpu-debug/
</code></pre>
<p>Among the characteristics of this project:</p>
<ul>
<li>Files have the <code>.cpp</code> extension instead of the <code>.cu</code> extension.</li>
<li>It compiles code for the native GPU architecture by default (so for the GPU of the computer you are compiling your code on).
This can easily be changed if you are cross-compiling by defining the flag <code>CMAKE_CUDA_ARCHITECTURES</code> at the configuration stage:</li>
</ul>
<pre><code>cmake -DCMAKE_CUDA_ARCHITECTURES=70 -DCMAKE_BUILD_TYPE=Release -Bbuild/gpu-release
</code></pre>
<ul>
<li>Several useful options inherited from cuda-battery (enabling C++20 and constexpr extension).</li>
<li>A testing framework where you can write your CPU tests using Google Test framework (see <code>demo/tests/demo_test.cpp</code>) and your hand-made GPU tests (see <code>demo/tests/demo_test_gpu.cpp</code>).</li>
<li>Moreover, when testing GPU code, we verify there is no memory leaks or some data races (using <code>compute-sanitizer</code>).</li>
</ul>
<p>We have documented the <a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/CMakeLists.txt">CMakeLists.txt</a> so you can adjust it to your own project.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="in-kernel-memory-allocation"><a class="header" href="#in-kernel-memory-allocation">In-Kernel Memory Allocation</a></h1>
<p>(<a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/src/inkernel_allocation.cpp">code of this section</a>)</p>
<p>The typical way to allocate data structures shared by all threads or blocks in a CUDA kernel is to get all the memory allocations done before the kernel starts.
However, when developing a larger CUDA kernel, it is frequent to rely on intermediate structures that are only temporary and are deleted before the end of the kernel.
It might be difficult, or at best inconvenient, to allocate everything before the start of the kernel.
Moreover, to follow good software engineering practice, these temporary structures should be hidden from the user of the kernel.</p>
<h2 id="thread-local-memory"><a class="header" href="#thread-local-memory">Thread-local Memory</a></h2>
<p>Thread-local memory is a chunk of memory that is only accessed by a single thread.
This is the default allocation mode when declaring a variable in CUDA kernel, hence it does not need any support from this library.</p>
<h2 id="block-local-memory"><a class="header" href="#block-local-memory">Block-local Memory</a></h2>
<p>Block-local memory is a chunk of memory that is only accessed by the threads of a single block.
To avoid any ambiguity: &quot;block-local memory&quot; is conceptual and might reside in global memory; it is not necessarily in shared memory.</p>
<p>A simple use-case is when blocks need to work on different copies of an original array.
Suppose <code>*v_ptr</code> is an array shared by all blocks.
In the following kernel, we show how to use <code>battery::make_unique_block</code> to copy <code>*v_ptr</code> into a block-local vector <code>v_block</code>.</p>
<pre><code class="language-c++">using gvector = battery::vector&lt;int, battery::global_allocator&gt;;
__global__ void block_vector_copy(mvector* v_ptr) {
  battery::unique_ptr&lt;gvector, battery::global_allocator&gt; v_block_ptr;
  gvector&amp; v_block = battery::make_unique_block(v_block_ptr, *v_ptr);
  // Now each block has its own local copy of the vector `*v_ptr`.
  // ...
  // We must synchronize the threads at the end, in case the thread holding the pointer in `unique_ptr` terminates before the other.
  cooperative_groups::this_thread_block().sync(); // Alternatively, `__syncthreads();`
}
</code></pre>
<p>Without this facility, we would need to initialize <code>n</code> copies of the vector in the host code and pass them as parameters to the kernel.
Finally, the function <code>make_unique_block</code> synchronizes all threads of the current block before returning, therefore <code>v_block</code> is directly usable by all threads.
Before you use this technique, keep reading because you might need to increase the size of the heap and stack.</p>
<h2 id="avoiding-obscure-cuda-runtime-errors"><a class="header" href="#avoiding-obscure-cuda-runtime-errors">Avoiding Obscure CUDA Runtime Errors</a></h2>
<p>Developing an entire system within a single kernel can easily lead to CUDA runtime error due to overflow of the allowed heap and stack memory.
The heap memory is by-default limited to 8 MB for allocations taking place in the kernel.
If you allocate more than 8 MB, which is not very difficult, you will run into an error of the style &quot;CUDA runtime error an illegal memory access was encountered&quot;.
In that case, you must increase the size of the heap, and this can be done as follows:</p>
<pre><code class="language-c++">// Multiply by 10 the default value, so now we have 80MB.
void increase_heap_size() {
  size_t max_heap_size;
  cudaDeviceGetLimit(&amp;max_heap_size, cudaLimitMallocHeapSize);
  CUDAE(cudaDeviceSetLimit(cudaLimitMallocHeapSize, max_heap_size*10));
  cudaDeviceGetLimit(&amp;max_heap_size, cudaLimitMallocHeapSize);
  printf(&quot;%%GPU_max_heap_size=%zu (%zuMB)\n&quot;, max_heap_size, max_heap_size/1000/1000);
}

int main() {
  increase_heap_size();
  auto vptr = battery::make_unique&lt;mvector, battery::managed_allocator&gt;(100000, 42);
  auto ptr = vptr.get();

  block_vector_copy&lt;&lt;&lt;256, 256&gt;&gt;&gt;(ptr);
  CUDAEX(cudaDeviceSynchronize());
}
</code></pre>
<p>For the stack, which is allocated per-thread, the problem can quickly arrive if you have many function calls and local variables.
In that case you can increase the size of the stack as follows:</p>
<pre><code class="language-c++">void increase_stack_size() {
  size_t max_stack_size = 1024;
  CUDAE(cudaDeviceSetLimit(cudaLimitStackSize, max_stack_size*10));
  cudaDeviceGetLimit(&amp;max_stack_size, cudaLimitStackSize);
  printf(&quot;%%GPU_max_stack_size=%zu (%zuKB)\n&quot;, max_stack_size, max_stack_size/1000);
}
</code></pre>
<p>For information, the stack frames are stored in global memory, but the compiler will try its best to place them in the registers and caches when possible.</p>
<h2 id="grid-local-memory"><a class="header" href="#grid-local-memory">Grid-local Memory</a></h2>
<p>Similarly to the previous section, we sometimes wish to initialize, inside the kernel, data that is shared by all blocks.
Once again, we suppose to have an original array <code>*v_ptr</code> that we wish to copy, but per-grid and not per-block.</p>
<pre><code class="language-c++">__global__ void grid_vector_copy(mvector* v_ptr) {
  battery::unique_ptr&lt;gvector, battery::global_allocator&gt; v_copy_ptr;
  gvector&amp; v_copy = battery::make_unique_grid(v_copy_ptr, *v_ptr);
  // `v_copy` is now accessible by all blocks.
  // ...
  // Same as with block-local memory, we want to guard against destructing the pointer too early.
  cooperative_groups::this_grid().sync();
}
</code></pre>
<p>To synchronize among threads, both <code>make_unique_block</code> and <code>make_unique_grid</code> rely on <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups">cooperative groups</a>.
In the case of <code>make_unique_grid</code>, CUDA requires the kernel to be launched with a <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#grid-synchronization">different syntax</a>:</p>
<pre><code class="language-c++">int main() {
  increase_heap_size();
  auto vptr = battery::make_unique&lt;mvector, battery::managed_allocator&gt;(100000, 42);
  auto ptr = vptr.get();
  void *kernelArgs[] = { &amp;ptr }; // be careful, we need to take the address of the parameter we wish to pass.
  dim3 dimBlock(256, 1, 1);
  dim3 dimGrid(256, 1, 1);
  cudaLaunchCooperativeKernel((void*)grid_vector_copy, dimGrid, dimBlock, kernelArgs);
  CUDAE(cudaDeviceSynchronize());
  return 0;
}
</code></pre>
<p>I am not sure why the syntax is different, but since it is a fairly recent feature, it might be improved in future releases.</p>
<h2 id="multi-grid-memory"><a class="header" href="#multi-grid-memory">Multi-grid Memory</a></h2>
<p>For now, we do not support multi-grid memory allocation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="shared-memory-allocator"><a class="header" href="#shared-memory-allocator">Shared Memory Allocator</a></h1>
<p>(<a href="https://github.com/lattice-land/cuda-battery/tree/v1.0.0/demo/src/demo.cpp">code of this section</a>)</p>
<p>We show how to use shared memory using a memory pool allocator.</p>
<pre><code class="language-c++">using pvector = battery::vector&lt;int, battery::pool_allocator&gt;;

__global__ void map_kernel_shared(mvector* v_ptr, size_t shared_mem_capacity) {
  // I. Create a pool of shared memory.
  extern __shared__ unsigned char shared_mem[];
  battery::unique_ptr&lt;battery::pool_allocator, battery::global_allocator&gt; pool_ptr;
  // /!\ We must take a reference to the pool_allocator to avoid copying it, because its copy-constructor is not thread-safe! (It can only be used by one thread at a time).
  battery::pool_allocator&amp; shared_mem_pool = battery::make_unique_block(pool_ptr, static_cast&lt;unsigned char*&gt;(shared_mem), shared_mem_capacity);

  // II. Transfer from global memory to shared memory.
  battery::unique_ptr&lt;pvector, battery::global_allocator&gt; shared_vector;
  size_t chunk_size = chunk_size_per_block(*v_ptr, gridDim.x);
  auto span = make_safe_span(*v_ptr, chunk_size * blockIdx.x, chunk_size);
  pvector&amp; v = battery::make_unique_block(shared_vector, span.data(), span.size(), shared_mem_pool);

  // III. Run the algorithm on the shared memory.
  block_par_map(v, [](int x){ return x * 2; }, blockDim.x, threadIdx.x);
  __syncthreads();

  // IV. Transfer back from shared memory to global memory.
  for(int i = threadIdx.x; i &lt; v.size(); i += blockDim.x) {
    (*v_ptr)[chunk_size * blockIdx.x + i] = v[i];
  }
}
</code></pre>
<p>We initialize one pool allocator per-block using the same technique as shown before.
However, we must be careful to take the <code>pool_allocator</code> by reference because its copy-constructor is not thread-safe; similarly to <code>shared_ptr</code> it maintains a shared counter to its memory pool.
Since memory allocation is done by only one thread anyway, it does not make sense to have multiple copies of this allocator.</p>
<p>The next step is to transfer the vector to the shared memory.
Each block works on a chunk of the initial array.
Therefore, we do not want to move the whole array in shared memory, but only the part of interest for that block.
To achieve that, we compute the size of the chunk using <code>chunk_size_per_block</code> and create a view of the vector covering only that chunks.
We use <a href="https://en.cppreference.com/w/cpp/container/span">std::span</a> to model the view.
Note that we can use the STL <code>std::span</code> because all its methods are <code>constexpr</code>, and we have the NVCC flag <code>--expt-relaxed-constexpr</code> inherited from cuda-battery.
We then copy this span into the vector <code>v</code> which is in shared memory.</p>
<p>The algorithm <code>block_par_map</code> is then called on <code>v</code> for each block.
The last step is to transfer the memory from the shared memory to the global memory.
After each block finished computing, the <code>map</code> function has been applied to each element of the initial vector.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="a-word-of-caution-on-shared-state-parallelism"><a class="header" href="#a-word-of-caution-on-shared-state-parallelism">A Word of Caution on Shared-State Parallelism</a></h1>
<p>Developing a parallel algorithm is relatively easy as long as threads read and write independent memory chunks.
Such an algorithm is referred as <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a>.
As a rule of thumb, it is best if you can stay within this paradigm.
For instance using map-reduce operations, where all threads work on separate memory cells during the map operation, and then a single thread performs a reduction of all temporary results obtained.
Sometimes this is not possible, or you want to get to the next level to get better performance.
Beware, designing <em>correct and efficient</em> parallel algorithms with threads communicating in a shared memory system is a whole new world.
We cannot even scratch the surface here, and this library does not provide much support for it.
Nevertheless, it is possible to write such algorithms in CUDA using <a href="https://nvidia.github.io/libcudacxx/extended_api/synchronization_primitives.html">atomics and synchronization primitives</a>.
To use them well, I would advise to take a class (or <a href="https://arxiv.org/abs/1701.00854">read a book</a>) on parallel programming and memory consistency models.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="turbo"><a class="header" href="#turbo">Turbo</a></h1>
<p>(For a gentle and quick introduction to GPU programming, you can check out my <a href="https://ulhpc-tutorials.readthedocs.io/en/latest/gpu/cuda2023/">tutorial</a> and go through the slides; it is especially useful to know the CUDA vocabulary).</p>
<p><em>30 August 2023.</em> Turbo is a constraint solver running entirely on the GPU with an architecture based on abstract domains.
Intuitively, it seems that constraint solvers are a poor match for GPUs.
These solvers have complex architecture with many dynamic data structures, divergent branches and often save and restore the memory when backtracking.
Clearly not the kind of things GPUs like the most.
Moreover, over the years, new solving techniques have been primarily developed within a sequential mindset, without consideration for parallel execution.
The most successful approach to parallel constraint solving is <em>embarrassingly parallel search</em> where the problem is decomposed into many subproblems that are solved independently on different cores.
Although GPUs have thousands of cores, it is not reasonable to execute one subproblem per core.
The reason is that a single core does not have its own cache.
Cores are actually grouped in streaming multiprocessors (SMs), and all cores in one SM share the same cache (called L1 cache).
The L1 cache is only 256KB on the most recent NVIDIA Hopper GPUs (which I don't have).
A quick calculation shows why one subproblem per core is not a viable option.
Suppose the domain of a variable is represented by an interval (the lightest you can do, it is usually represented by a set of intervals), hence 8 bytes, a store of 1000 variables takes 8KB of memory.
An SM has 64 (integer) cores, hence we need 64*8 = 512KB of cache to fit all the stores at once.
We should also store the propagators, and although the stateless propagators can be shared among subproblems, it easily takes up dozens of KB; memory that will not be available for the stores.
The threads will end up competing for the caches, and it will generate many memory transfers between the global memory, L2 cache and L1 cache.
Memory transfers are usually the most costly operation, hence you try to avoid them in a GPU; loading data from the global memory is about 10x slower than from the L1 cache.
I am not saying it is impossible to do efficiently, but it seems quite complicated, especially when taking into account warps that are SIMD units of the GPUs (blocks of 32 threads), and thus must all have their data ready in the cache at the same time when executing.
This is a hypothesis based on intuition and discussion with GPU experts, but I should verify it experimentally.</p>
<p>Instead of relying on &quot;one subproblem per core&quot; design that is so commonly used in CPU parallel constraint solver, we can execute one subproblem per SM.
And the parallelism inside an SM is obtained by parallelizing propagation!
It is worth mentioning that no attempt to parallelize propagation resulted in a faster solver in comparison to simply parallelizing the search.
In fact, no modern solver parallelizes propagation.
Because most solvers are designed for CPU, it seems that adapting an existing solver to work on GPU is doomed to fail.
The design rational of Turbo is to aim first at a <em>simple but correct</em> parallel design.</p>
<p>I write this technical journal to document my attempts to obtain an efficient GPU-based constraint solver named Turbo.
Moreover, because I like my life to be (too) complicated, Turbo is also based on abstract interpretation and implements so-called <em>abstract domains</em> which are very general mathematical generalizations of the representation of constraints.
Let's go!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="design-rational"><a class="header" href="#design-rational">Design Rational</a></h1>
<p><em>31 August 2023.</em> We start this adventure with a functional but slow GPU solver.
To make it work on the GPU, we had to simplify the architecture of mainstream CPU-based constraint solver in many regards.
Let's start with the current design decisions behind Turbo:</p>
<ul>
<li><em>Pure GPU</em>: We decided early on to make it a solver entirely on the GPU, this is to avoid memory transfers between CPU and GPU.
We are also forced to design each component with a parallel mindset.</li>
<li><em>Propagation</em>: The propagation loop is based on the very first AC1 algorithm: we simply take the first N propagators, run them in parallel, then take the nextÂ N propagators, and so on until they are all executed, where N is the number of cores available in the current block.
We repeat this operation in a fixed point loop until no change is detected anymore.
This implies we do not have a queue of propagators to be woken up or any notion of events in the solver.</li>
<li><em>Backtracking</em>: It is based on full recomputation, each time we backtrack, we re-apply all the decisions from the root node; actually, from the root of the current EPS subproblem being solved.</li>
<li><em>Embarrassingly parallel search</em>: We dynamically create subproblems that are explored individually by GPU blocks, hence the parallel propagation only happens inside one block.</li>
<li><em>Abstract domains</em>: The solver architecture is based on abstract domains which encapsulate solving algorithms dedicated to a particular constraint language, similarly to theories in SMT.
However, abstract domains are semantic objects while SMT theories are syntactic objects (and a formal connection exists between the two), which makes abstract domains closer the actual implementation code.</li>
<li><em>No conflict learning</em>: Conflicts require dynamic data structures to represent the implication graph, and memory allocation is quite costly on GPU, so we currently have no conflict learning.</li>
<li><em>Lock-free</em>: Although propagators are sharing the memory, we do not have locks on the variables, and actually no lock at all.</li>
</ul>
<p>We have thrown away many optimizations considered essential in mainstream CPU-based constraint solvers.
Incidentally, the GPU solver is much simpler than a CPU solver, while obtaining similar performances (of course, when comparing a pure CP solver without SAT learning).
Back in 2021, our hypothesis was that drastically simplifying the solver design was necessary to implement a GPU solver and that it could be efficient.
We implemented a <a href="https://github.com/ptal/turbo/tree/aaai2022">prototype (Turbo)</a> that demonstrated it could work as we obtained similar performances between Turbo and Gecode (on a 6-cores CPU with 12 threads) on RCPSP.
We also laid out the theory behind parallel constraint propagation on GPU and show that, although we did not use any lock, it was giving correct results.
These findings were summarized in our <a href="http://hyc.io/papers/aaai2022.pdf">AAAI 2022 paper</a>.</p>
<p>For more than one year, I have been refactoring and extending this initial prototype in multiple ways.
One goal has been achieved now and Turbo can solve MiniZinc model with two limitations: for now, it only supports integer variables, and all global constraints are decomposed into primitive constraints.
The solver Turbo is part of a larger project called <em>Lattice Land</em>, which is a collection of libraries representing lattice data structures.
The long-term goal is to be able to reuse these libraries outside of constraint solving, for instance in static analysis by abstract interpretation and in distributed computing using CRDT.
The code of Turbo is the glue putting together the required libraries to solve a constraint problem.</p>
<p>However, as one could expect, the new <em>abstract domain</em> design comes with a price: it is less efficient than the prototype.
I will try to integrate one by one various optimizations and attempt to first reach the previous efficiency, and then go beyond it.
To benchmark our progresses, I think it is reasonable to start with a set of easy instances and a set of hard instances.</p>
<ul>
<li><em>Resource-constrained project scheduling problem (RCPSP)</em>: Patterson instances (110 in total), these are all solved quickly by modern CP-SAT solvers.</li>
<li><em>MiniZinc competition 2022</em>: The problems are more diverse but also harder.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
